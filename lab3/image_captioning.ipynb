{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2s1A9eLRPEj"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VRLVEKiTEn04"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFwSaNB8jF7s"
      },
      "source": [
        "<style>\n",
        "td {\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        "th {\n",
        "  text-align: center;\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Image captioning with visual attention\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/image_captioning\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QASbY_HGo4Lq"
      },
      "source": [
        "Given an image like the example below, your goal is to generate a\n",
        "caption such as \"a surfer riding on a wave\".\n",
        "\n",
        "<table style=\"text-align: center;\">\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://tensorflow.org/images/surf.jpg\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A man surfing, from <a href=https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg>wikimedia</a></th>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "The model architecture used here is inspired by [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), but has been updated to use a 2-layer Transformer-decoder. To get the most out of this tutorial you should have some experience with [text generation](https://www.tensorflow.org/text/tutorials/text_generation),  [seq2seq models & attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention), or [transformers](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbD8n0w7d3F"
      },
      "source": [
        "The model architecture built in this tutorial is shown below. Features are extracted from the image, and passed to the cross-attention layers of the Transformer-decoder.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The model architecture</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://tensorflow.org/images/tutorials/transformer/ImageCaptioning.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IxifZKT6vXQ"
      },
      "source": [
        "The transformer decoder is mainly built from attention layers. It uses self-attention to process the sequence being generated, and it uses cross-attention to attend to the image.\n",
        "\n",
        "By inspecting the attention weights of the cross attention layers you will see what parts of the image the model is looking at as it generates words.\n",
        "\n",
        "![Prediction](https://tensorflow.org/images/imcap_prediction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87us2sLVdwME"
      },
      "source": [
        "This notebook is an end-to-end example. When you run the notebook, it downloads a dataset, extracts and caches the image features, and trains a decoder model. It then uses the model to generate captions on new images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bwwk4uxRz6A"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc06pTaBbl72",
        "outputId": "902fefed-78c9-4cba-e972-c2060c869f89"
      },
      "outputs": [],
      "source": [
        "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R1hQGtZEi8Y",
        "outputId": "7248002d-e53a-4268-ec85-82a29bec8e56"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y tensorflow estimator keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xbt8BkPv8Ou",
        "outputId": "a087c042-3daf-466d-8ac3-332afd39e91d"
      },
      "outputs": [],
      "source": [
        "# !pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGZmOuqMia9",
        "outputId": "ddfa3dc6-a446-4c66-e3c5-16789fc49e5f"
      },
      "outputs": [],
      "source": [
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ6q39Vd-y-7"
      },
      "source": [
        "This tutorial uses lots of imports, mostly for loading the dataset(s)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ozara\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu,True)\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl9qGnjWrv80"
      },
      "source": [
        "## [Optional] Data handling\n",
        "\n",
        "This section downloads a captions dataset and prepares it for training. It tokenizes the input text, and caches the results of running all the images through a pretrained feature-extractor model. It's not critical to understand everything in this section.\n",
        "\n",
        " <section class=\"expandable tfo-display-only-on-site\">\n",
        " <button type=\"button\" class=\"button-red button expand-control\">Toggle section</button>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAagBw5p-TM"
      },
      "source": [
        "#### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVnOzax5ZHe7",
        "outputId": "ff538d8b-95b4-49bb-aa8f-bef5fea90dee"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s_CWZcOZBye",
        "outputId": "8251e4a8-6284-4062-c426-ddb18cc8d818"
      },
      "outputs": [],
      "source": [
        "# Download image files\n",
        "# image_zip = tf.keras.utils.get_file('/content/drive/MyDrive/flickr30k-images-ecemod.zip',\n",
        "#                                       cache_subdir=os.path.abspath('.'),\n",
        "#                                       origin='https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip',\n",
        "#                                       extract=True)\n",
        "# os.remove(image_zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sCGnBOsOhY91"
      },
      "outputs": [],
      "source": [
        "use_drive=False #αλλαγη αν θελουμε drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "omMwenkTZBtk"
      },
      "outputs": [],
      "source": [
        "csv_path=\"\"\n",
        "if use_drive==True:\n",
        "  csv_path=\"/content/drive/MyDrive/\"\n",
        "\n",
        "# Download captions file\n",
        "captions_file = tf.keras.utils.get_file(csv_path+'captions_new.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/captions_new.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download train files list\n",
        "train_files_list = tf.keras.utils.get_file(csv_path+'train_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/train_files.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download test files list\n",
        "test_files_list = tf.keras.utils.get_file(csv_path+'test_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/test_files.csv',\n",
        "                                           extract=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ey_LOe6hZBq7"
      },
      "outputs": [],
      "source": [
        "if use_drive == True:\n",
        "  path=\"/content/drive/MyDrive/\"\n",
        "else: \n",
        "  path=\".\"\n",
        "IMAGE_DIR=\"image_dir\"\n",
        "path = pathlib.Path(path)\n",
        "   \n",
        "captions = (path/captions_file).read_text(\"UTF-8\").splitlines()\n",
        "captions = (line.split('\\t') for line in captions)\n",
        "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "   \n",
        "cap_dict = collections.defaultdict(list)\n",
        "for fname, cap in captions:\n",
        "  cap_dict[fname].append(cap)\n",
        "   \n",
        "train_files = (path/train_files_list).read_text(\"UTF-8\").splitlines()\n",
        "train_captions = [(str(IMAGE_DIR+ '/' +fname), cap_dict[fname]) for fname in train_files]\n",
        "   \n",
        "test_files = (path/test_files_list).read_text(\"UTF-8\").splitlines()\n",
        "test_captions = [(str(IMAGE_DIR+ '/' +fname), cap_dict[fname]) for fname in test_files]\n",
        "   \n",
        "train_raw = tf.data.experimental.from_list(train_captions)\n",
        "test_raw = tf.data.experimental.from_list(test_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAQSps5F8RQI",
        "outputId": "5e2185da-9099-4b68-adac-bc49a8bcaf30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_raw.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIa0ZaP4tBez",
        "outputId": "0cac37eb-cc50-4118-a38e-3a760cc38a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'image_dir/_3430497.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'The skier is wearing a yellow jumpsuit and sliding across a yellow rail .'\n",
            " b'A yellow uniformed skier is performing a trick across a railed object .'\n",
            " b'A skier in electric green on the edge of a ramp made of metal bars .'\n",
            " b'A person on skis on a rail at night .'\n",
            " b'A skier slides along a metal rail .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Image feature extractor\n",
        "\n",
        "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IlUckK8Zfikv"
      },
      "outputs": [],
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True)\n",
        "mobilenet.trainable=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dojkiou9gL3R"
      },
      "source": [
        "Here's a function to load an image and resize it for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JyQ7zS6gzZh"
      },
      "source": [
        "The model returns a feature map for each image in the input batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY86n2i6wJNm",
        "outputId": "4e445985-3017-4a8f-a7a7-e6601bae5cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 576)\n"
          ]
        }
      ],
      "source": [
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "\n",
        "  print(test_img_batch.shape)\n",
        "  print(mobilenet(test_img_batch).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Setup the text tokenizer/vectorizer\n",
        "\n",
        "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
        "\n",
        "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n",
        "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n",
        "* Create word-to-index and index-to-word mappings to display results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NroZIzB90hD3"
      },
      "outputs": [],
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n9SQOXFsyS36"
      },
      "outputs": [],
      "source": [
        "# Use the top 5000 words for a vocabulary.\n",
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRahTDtWhJIf",
        "outputId": "afcfb59a-daa1-464e-cbe8-27f9a1e962b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'and', 'man']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2mGxD33JCxN",
        "outputId": "8945616b-0d9a-43ac-a50d-fae1dea70a9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[3, 2, 755, 5, 2, 63, 4], [3, 2, 2866, 34, 4]]>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo-cfCX3LnHs",
        "outputId": "b6f859ce-031f-4456-a677-ee215713371c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n",
              " [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrUUfGc65vAT",
        "outputId": "4556a0ab-b591-40a2-d116-6803ccca918f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "### Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX0Z_98S2tN"
      },
      "source": [
        "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
        "\n",
        "This function will replicate the image so there are 1:1 images to captions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3_Lqwl9NiGT0"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZGUsuGzUfzt",
        "outputId": "e1d67a5d-c532-4699-f78f-e5f3c397ab57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENR_-swVhnm"
      },
      "source": [
        "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2DsgQ_hZT4C2"
      },
      "outputs": [],
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA1x2j0JXX-N"
      },
      "source": [
        "This function adds operations to a dataset. The steps are:\n",
        "\n",
        "1. Load the images (and ignore images that fail to load).\n",
        "2. Replicate images to match the number of captions.\n",
        "3. Shuffle and rebatch the `image, caption` pairs.\n",
        "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
        "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4_Pt9zldjQ0q"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrQ85t1GNfpQ"
      },
      "source": [
        "You could install the feature extractor in your model and train on the datasets like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KlhOG5cjQ0r",
        "outputId": "4d5f8a7e-b3ff-4078-e9bb-e6725bd67e14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7Zy9F3zX7i2",
        "outputId": "29a958fe-d2e9-48ba-d4a6-4cc9e69cf061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZyKygJ8S8zW"
      },
      "source": [
        "### [Optional] Cache the image features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHKhSKhti6NS"
      },
      "source": [
        "Since the image feature extractor is not changing, and this tutorial is not using image augmentation, the image features can be cached. Same for the text tokenization. The time it takes to set up the cache is earned back on each epoch during training and validation. The code below defines two functions `save_dataset` and `load_dataset`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9N1MX5ym6xm5"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  # Run the feature extractor on each batch\n",
        "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
        "  def gen():\n",
        "    for (images, captions) in tqdm.tqdm(ds): \n",
        "      feature_maps = image_model(images)\n",
        "\n",
        "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
        "      yield feature_maps, captions\n",
        "\n",
        "  # Wrap the generator in a new tf.data.Dataset.\n",
        "  new_ds = tf.data.Dataset.from_generator(\n",
        "      gen,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=image_model.output_shape),\n",
        "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
        "\n",
        "  # Apply the tokenization \n",
        "  new_ds = (new_ds\n",
        "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "            .unbatch()\n",
        "            .shuffle(1000))\n",
        "\n",
        "  # Save the dataset into shard files.\n",
        "  def shard_func(i, item):\n",
        "    return i % shards\n",
        "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
        "\n",
        "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1000)\n",
        "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
        "  \n",
        "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_index(i, x):\n",
        "    return x\n",
        "\n",
        "  ds = (ds\n",
        "        .map(drop_index, tf.data.AUTOTUNE)\n",
        "        .shuffle(shuffle)\n",
        "        .padded_batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNdzrenxB3Yy",
        "outputId": "05c09217-71f9-4923-8c6b-b255a2d93c35"
      },
      "outputs": [],
      "source": [
        "# save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
        "# save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI265LiDslr2"
      },
      "source": [
        "## Data ready for training\n",
        "\n",
        "After those preprocessing steps, here are the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = load_dataset('train_cache')\n",
        "test_ds = load_dataset('test_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B80JXj7HloX",
        "outputId": "6325d40f-5fc3-4955-e438-2509ad0c5fbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jfb8qknlsKi"
      },
      "source": [
        "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJBEwuXLZQdw",
        "outputId": "9a17203a-3b36-4760-eedb-bbfd6c0e6c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 7, 7, 576)\n",
            "(32, 43)\n",
            "(32, 43)\n"
          ]
        }
      ],
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22R58DzZoF17"
      },
      "source": [
        "The input tokens and the labels are the same, just shifted by 1 step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7h5UGftn1hT",
        "outputId": "6e6cbc82-1e4a-44c9-f428-daafb20e3203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   3    9   12   28   22    8   27  353   12   43 1218  139  119  712\n",
            "    5    6 1557    5    6  242   11    2   36   25    2   83   10  775\n",
            "    5    6   86    8   44   10 2415    0    0    0    0    0    0    0\n",
            "    0]\n",
            "[   9   12   28   22    8   27  353   12   43 1218  139  119  712    5\n",
            "    6 1557    5    6  242   11    2   36   25    2   83   10  775    5\n",
            "    6   86    8   44   10 2415    4    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ],
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfICM49WFpIb"
      },
      "source": [
        "## A Transformer decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyjuWsmZoyO"
      },
      "source": [
        "This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n",
        "\n",
        "The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The Transformer encoder and decoder.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRXWwIKNybB"
      },
      "source": [
        "The model will be implemented in three main parts: \n",
        "\n",
        "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
        "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
        "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
        "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
        "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
        "1. Output - A multiclass-classification over the output vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngm3SQMCaYU"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9suaARZGPKw"
      },
      "source": [
        "The input text has already been split up into tokens and converted to sequences of IDs. \n",
        "\n",
        "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
        "\n",
        "The `SeqEmbedding` layer defined below:\n",
        "\n",
        "- It looks up the embedding vector for each token.\n",
        "- It looks up an embedding vector for each sequence location.\n",
        "- It adds the two together.\n",
        "- It uses `mask_zero=True` to initialize the keras-masks for the model.\n",
        "\n",
        "Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gensim.downloader\n",
        "# # Show all available models in gensim-data\n",
        "# # print(list(gensim.downloader.info()['models'].keys()))\n",
        "\n",
        "# glove_vectors100 = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "# import joblib\n",
        "# joblib.dump(glove_vectors100, 'glove_vectors100.plk')\n",
        "import joblib\n",
        "glove_vectors100 = joblib.load('glove_vectors100.plk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 4933 words (67 misses)\n"
          ]
        }
      ],
      "source": [
        "myvoc = tokenizer.get_vocabulary()\n",
        "myword_index = dict(zip(myvoc, range(len(myvoc))))\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((len(myvoc), embedding_dim))\n",
        "for word, i in myword_index.items():\n",
        "    try:\n",
        "        embedding_vector = glove_vectors100.get_vector(word)\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    except:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "P91LU2F0a9Ga"
      },
      "outputs": [],
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "    \n",
        "    # self.token_embedding = tf.keras.layers.Embedding(\n",
        "    #   input_dim=vocab_size,\n",
        "    #   output_dim=depth,\n",
        "    #   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    #   trainable=False,\n",
        "    #   mask_zero=True\n",
        "    # )\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        mask_zero=True)\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II1mD-bBCdMB"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMLeMtKPTCW"
      },
      "source": [
        "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
        "\n",
        "The `CausalSelfAttention` layer is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c66OTRwQfd8"
      },
      "source": [
        "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rIY6Vu2pLBAO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hn5p6f-RE0C"
      },
      "source": [
        "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cWKrl7teOnH2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXoiVNPRoJc"
      },
      "source": [
        "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ydcW5KZZHou7"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgbYrF5Csqu"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnKZkrklAQf"
      },
      "source": [
        "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WQD87efena5"
      },
      "source": [
        "But there are a few other features you can add to make this work a little better:\n",
        "\n",
        "1. **Handle bad tokens**: The model will be generating text. It should\n",
        "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
        "   `'[START]'`). So set the bias for these to a large negative value.\n",
        "\n",
        "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
        "\n",
        "2. **Smart initialization**: The default initialization of a dense layer will\n",
        "  give a model that initially predicts each token with almost uniform\n",
        "  likelihood. The actual token distribution is far from uniform. The\n",
        "  optimal value for the initial bias of the output layer is the log of the\n",
        "  probability of each token. So include an `adapt` method to count the tokens\n",
        "  and set the optimal initial bias. This reduces the initial loss from the\n",
        "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
        "  entropy of the distribution (`-p*log(p)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CeWw2SFDHUfo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQHqANd1A6Q"
      },
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGnOQyc501B2",
        "outputId": "adcea29c-e63a-49b5-fab3-b83af09a57ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3282/3282 [01:44<00:00, 31.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uniform entropy: 8.52\n",
            "Marginal entropy: 5.47\n"
          ]
        }
      ],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gq-ICN7bD-u"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou4fPH_SWgH"
      },
      "source": [
        "To build the model, you need to combine several parts:\n",
        "\n",
        "1. The image `feature_extractor` and the text `tokenizer` and.\n",
        "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
        "   vectors `(batch, sequence, channels)`.\n",
        "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
        "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bHCISYehH1f6"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        depth=units,\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW390dOz9T-x"
      },
      "source": [
        "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
        "\n",
        "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
        "* If the text has dtype `tf.string` run it through the tokenizer.\n",
        "\n",
        "After that running the model is only a few steps:\n",
        "\n",
        "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
        "2. Look up the token embeddings.\n",
        "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
        "4. Run the output layer to predict the next token at each position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lPdb7I4h9Ulo"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def call(self, inputs):\n",
        "  image, txt = inputs\n",
        "\n",
        "  if image.shape[-1] == 3:\n",
        "    # Apply the feature-extractor, if you get an RGB image.\n",
        "    image = self.feature_extractor(image)\n",
        "  \n",
        "  # Flatten the feature map\n",
        "  image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "  if txt.dtype == tf.string:\n",
        "    # Apply the tokenizer if you get string inputs.\n",
        "    txt = tokenizer(txt)\n",
        "\n",
        "  txt = self.seq_embedding(txt)\n",
        "\n",
        "  # Look at the image\n",
        "  for dec_layer in self.decoder_layers:\n",
        "    txt = dec_layer(inputs=(image, txt))\n",
        "    \n",
        "  txt = self.output_layer(txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kmM7aZQsLiyU"
      },
      "outputs": [],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "### Generate captions\n",
        "\n",
        "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
        "\n",
        "Start by downloading a test image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cwFcdMqC-jE2"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBIiTkubmxA"
      },
      "source": [
        "To caption an image with this model:\n",
        "\n",
        "- Extract the `img_features`\n",
        "- Initialize the list of output tokens with a `[START]` token.\n",
        "- Pass `img_features` and `tokens` into the model.\n",
        "  - It returns a list of logits.\n",
        "  - Choose the next token based on those logits.  \n",
        "  - Add it to the list of tokens, and continue the loop.\n",
        "  - If it generates an `'[END]'` token, break out of the loop.\n",
        "\n",
        "So add a \"simple\" method to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Nf1Jie9ef_Cg"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    # print()\n",
        "    # print(preds.shape)\n",
        "    # for i in len(preds[0]):\n",
        "    #   print(preds)\n",
        "    # print(tokens, \"HERE\")\n",
        "    # print()\n",
        "    # if(n==3): break\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN2NPX2zB8y"
      },
      "source": [
        "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPm96CccvHnq",
        "outputId": "16e8d091-425a-4155-e996-18e85e8a7eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a a\n",
            "a\n",
            "from with bends trees on with person a sitting bleachers with chalk arranged before everywhere men\n"
          ]
        }
      ],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JefwCRZ8z-Ah"
      },
      "source": [
        "The temperature parameter allows you to interpolate between 3 modes:\n",
        "\n",
        "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
        "2. Random sampling according to the logits (`temperature=1.0`).\n",
        "3. Uniform random sampling (`temperature >> 1.0`). \n",
        "\n",
        "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0FpTvaPkqON"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcwZdqObK-U"
      },
      "source": [
        "To train the model you'll need several additional components:\n",
        "\n",
        "- The Loss and metrics\n",
        "- The Optimizer\n",
        "- Optional Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IW2mWa2sAG"
      },
      "source": [
        "### Losses and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbpbDQTw1lOW"
      },
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "s24im3FqxAfT"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhjHqgv3F2e"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dyQN9UfJYEd"
      },
      "source": [
        "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "IKDwbZOCZ-AP"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNA3_RAsdl0"
      },
      "source": [
        "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGVLpzo13rcA",
        "outputId": "699a04cb-ddca-4190-d066-abf3fdb89b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "a a\n",
            "a on is a a a a a\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxp4KZRKDk9"
      },
      "source": [
        "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MjzrwGZp23xx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaJhQpcG8u0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBXG0dCDKO55"
      },
      "source": [
        "Configure and execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2OR5ZpAII__u"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro955bQ2KR0X"
      },
      "source": [
        "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
        "\n",
        "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aB0baOVMZe9",
        "outputId": "2e3aed73-a2d6-4faf-ea59-416555b7a982"
      },
      "outputs": [],
      "source": [
        "# history = model.fit(\n",
        "#     train_ds.repeat(),\n",
        "#     steps_per_epoch=100,\n",
        "#     validation_data=test_ds.repeat(),\n",
        "#     validation_steps=20,\n",
        "#     epochs=100,\n",
        "#     callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P634LfVgw-eV"
      },
      "source": [
        "Plot the loss and accuracy over the training run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "6Wn8KSkUw916",
        "outputId": "a14f24ca-bde0-465d-e931-626bb7dfaad9"
      },
      "outputs": [],
      "source": [
        "# plt.plot(history.history['loss'], label='loss')\n",
        "# plt.plot(history.history['val_loss'], label='val_loss')\n",
        "# plt.ylim([0, max(plt.ylim())])\n",
        "# plt.xlabel('Epoch #')\n",
        "# plt.ylabel('CE/token')\n",
        "# plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "yZQ78b2Kxw-T",
        "outputId": "2676b6eb-58aa-4454-c999-d71aa689f41d"
      },
      "outputs": [],
      "source": [
        "# plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "# plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "# plt.ylim([0, max(plt.ylim())])\n",
        "# plt.xlabel('Epoch #')\n",
        "# plt.ylabel('CE/token')\n",
        "# plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQN1qT7KNqbL"
      },
      "source": [
        "## Attention plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XJaC2b2J23"
      },
      "source": [
        "Now, using the trained model,  run that `simple_gen` method on the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1UQPtNTb2eu3",
        "outputId": "9b02be67-10a9-40a9-af9b-5dd564a647ac"
      },
      "outputs": [],
      "source": [
        "# result = model.simple_gen(image, temperature=0.0)\n",
        "# result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXbmeLGN1bJ"
      },
      "source": [
        "Split the output back into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zHKOpm0w5Xto"
      },
      "outputs": [],
      "source": [
        "# str_tokens = result.split()\n",
        "# str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-AjuAV55Qo"
      },
      "source": [
        "The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZpyuQvq2q-B",
        "outputId": "442b0ada-8a06-4cf1-9591-4907fefc47ac"
      },
      "outputs": [],
      "source": [
        "# attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "# [map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42ImsWv6oHG"
      },
      "source": [
        "So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ojwtvnkh6mS-"
      },
      "outputs": [],
      "source": [
        "# attention_maps = tf.concat(attn_maps, axis=0)\n",
        "# attention_maps = einops.reduce(\n",
        "#     attention_maps,\n",
        "#     'batch heads sequence (height width) -> sequence height width',\n",
        "#     height=7, width=7,\n",
        "#     reduction='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TM7rA3zGpJW"
      },
      "source": [
        "Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWmWerGCZp3",
        "outputId": "13fa271e-58bf-470f-e8f9-ff666ffe7e37"
      },
      "outputs": [],
      "source": [
        "# einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7XYGFUd-U7"
      },
      "source": [
        "So here is where the model was focusing attention while generating each token of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "    \n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "PI4NAAws9rvY",
        "outputId": "6d3082e3-08bc-4833-864b-ad45bcb32ab7"
      },
      "outputs": [],
      "source": [
        "# plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTz0abQKMkV"
      },
      "source": [
        "Now put that together into a more usable function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "mktpfW-SKQIJ"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "  \n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "FntRkY11OiMw",
        "outputId": "8fce596e-6fc9-459f-b76a-ceb3a0a61b97"
      },
      "outputs": [],
      "source": [
        "# run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "## Try it on your own images\n",
        "\n",
        "For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "9Psd1quzaAWg",
        "outputId": "7c97d3b5-4825-430d-ac56-8879fc0a9bf2"
      },
      "outputs": [],
      "source": [
        "# image_url = 'https://drive.google.com/file/d/1D_gEsqA30YlRX7JiT6Q-chEXch328Q0Z/download?usp=sharing'\n",
        "# image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "\n",
        "# condom man\n",
        "# image = load_image('./image_dir/_323308.jpg')\n",
        "\n",
        "# run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScGuzPQ32rB0"
      },
      "source": [
        "## Blue Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAiMKoFk2zoK",
        "outputId": "e60a2117-7379-42af-d033-51b60de4cee9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from fractions import Fraction\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu ,SmoothingFunction\n",
        "\n",
        "'image_dir/_222324491.jpg'\n",
        "image = load_image('image_dir/_222324491.jpg')\n",
        "def my_sentence_blue(image, my_model):\n",
        "  decoded_captioning = my_model.simple_gen(image).split()\n",
        "\n",
        "  captions_true = [item for item in train_captions if item[0] == 'image_dir/_222324491.jpg']\n",
        "\n",
        "  #split different captions in order to be processed by the bleu function\n",
        "  captions_reference = [caption[:-1].split() for caption in captions_true[0][1]]\n",
        "\n",
        "  return sentence_bleu(captions_reference, decoded_captioning, weights=(0.4, 0.3, 0.2, 0.1), smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "def my_corpus_blue(image_count, my_model, mytemperature=1):\n",
        "# corpus bleu for all test set\n",
        "  decoded_captions = []\n",
        "  captions_reference = []\n",
        "  for img in test_captions[:image_count]:\n",
        "    image = load_image(img[0]) # get directory from captions\n",
        "    decoded_captions.append(my_model.simple_gen(image, temperature=mytemperature).split())\n",
        "\n",
        "    # captions_reference.append([caption.lower()[:-1].split(\" \") for caption in img[1]])\n",
        "    captions_reference.append([caption.lower().split(' ')[:-1] for caption in img[1]])\n",
        "\n",
        "  return corpus_bleu(captions_reference, decoded_captions, weights=(0.4, 0.3, 0.2, 0.1), smoothing_function=SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25118864315095807\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "print(nltk.translate.bleu_score.sentence_bleu([[\"hello\", \"hello\"],[\"goof\"]],[\"hello\",\"goof\"], weights=(0.4, 0.3, 0.2, 0.1), smoothing_function=SmoothingFunction().method1))\n",
        "\n",
        "print(nltk.translate.bleu_score.corpus_bleu([[\"hello\", \"hello\"],[\"goof\"]],[\"hello\",\"goof\"], weights=(0.4, 0.3, 0.2, 0.1), smoothing_function=SmoothingFunction().method1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0977767686209551"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_captions[0][1][:-1]\n",
        "captions_true = [item for item in train_captions if item[0] == 'image_dir/_222324491.jpg']\n",
        "captions_reference = [caption[:-1].split() for caption in captions_true[0][1]]\n",
        "sentence_bleu(captions_reference[1:4], captions_reference[0], weights=(0.4, 0.3, 0.2, 0.1), smoothing_function=SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZmaVkpDSRSS"
      },
      "source": [
        "## Βελτίωση Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "al-h9U7lPUwk"
      },
      "outputs": [],
      "source": [
        "vgg19=tf.keras.applications.VGG19(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation=\"softmax\"\n",
        ")\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Προεπεξεργασία Κειμένου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cap_lens = []\n",
        "for my_caption in train_captions:\n",
        "    for x in my_caption[1]:\n",
        "        cap_lens.append(len(x.split())-1) \n",
        "\n",
        "plt.hist(cap_lens,bins=50, rwidth=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "captions = (path/captions_file).read_text(\"UTF-8\").splitlines()\n",
        "captions = (line.split('\\t') for line in captions)\n",
        "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "reduced_cap_dict = collections.defaultdict(list)\n",
        "for fname, cap in captions:\n",
        "  if (len(cap.split()[:-1]) <= 25) and (len(cap.split()[:-1])>=5):\n",
        "    reduced_cap_dict[fname].append(cap)\n",
        "\n",
        "\n",
        "for name,caption_list in reduced_cap_dict.items():\n",
        "  list_len = len(caption_list)\n",
        "  if(list_len<5):\n",
        "    for i in range(5-list_len):\n",
        "      reduced_cap_dict[name].append(reduced_cap_dict[name][list_len-1])\n",
        "\n",
        "\n",
        "reduced_train_captions = [(str(IMAGE_DIR+ '/' +fname), reduced_cap_dict[fname]) for fname in train_files]\n",
        "   \n",
        "reduced_test_captions = [(str(IMAGE_DIR+ '/' +fname), reduced_cap_dict[fname]) for fname in test_files]\n",
        "   \n",
        "reduced_train_raw = tf.data.experimental.from_list(reduced_train_captions)\n",
        "reduced_test_raw = tf.data.experimental.from_list(reduced_test_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduced_train_raw.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'image_dir/_3430497.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'The skier is wearing a yellow jumpsuit and sliding across a yellow rail .'\n",
            " b'A yellow uniformed skier is performing a trick across a railed object .'\n",
            " b'A skier in electric green on the edge of a ramp made of metal bars .'\n",
            " b'A person on skis on a rail at night .'\n",
            " b'A skier slides along a metal rail .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for ex_path, ex_captions in reduced_train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqE0lEQVR4nO3df1RVdb7/8ReI/NAERC8cuCLSj/FHoqYW0Q+nkiUaNTp5Z7IonYl0aqAJndH0XkPTZjRMM82rNZNSNy11rTTTLkqYWoqoKDclh9EuqY0duDMGxx8pKPv7R1/28sgvQX5+fD7W2mt59n7vz/58zmef1avN2Wd7WJZlCQAAwDCeLd0BAACApkDIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYyaulO9CSKioqdPLkSXXq1EkeHh4t3R0AAHAVLMvS6dOnFRYWJk/Pmq/XXNch5+TJkwoPD2/pbgAAgAY4ceKEunXrVuP26zrkdOrUSdKPb5K/v38L9wYAAFwNl8ul8PBw+7/jNbmuQ07ln6j8/f0JOQAAtDF1fdWELx4DAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGMmrpTsAtBY9pm6qsu6bufEt0BMAQGPgSg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI/EUcrR5PD0cAFAdruQAAAAjEXIAAICRCDkAAMBI9Q45O3bs0MMPP6ywsDB5eHho/fr1btsty1JqaqpCQ0Pl5+en2NhYHTlyxK3m1KlTSkhIkL+/vwIDA5WYmKgzZ8641Xz55Ze699575evrq/DwcKWlpVXpy9q1a9WrVy/5+voqKipKn3zySX2HAwAADFXvkHP27Fn1799fS5YsqXZ7WlqaFi1apGXLliknJ0cdO3ZUXFyczp8/b9ckJCQoPz9fmZmZ2rhxo3bs2KEJEybY210ul4YNG6aIiAjl5uZq3rx5mjlzpt566y27ZteuXXrssceUmJioAwcOaNSoURo1apQOHTpU3yEBAAADeViWZTV4Zw8PrVu3TqNGjZL041WcsLAw/f73v9cf/vAHSVJpaalCQkKUnp6uMWPG6PDhw+rTp4/27t2rwYMHS5IyMjL04IMP6ttvv1VYWJiWLl2q//iP/5DT6ZS3t7ckaerUqVq/fr3++te/SpIeffRRnT17Vhs3brT7c+edd2rAgAFatmzZVfXf5XIpICBApaWl8vf3b+jbgBbWWHdXcZcWALQNV/vf70b9Tk5hYaGcTqdiY2PtdQEBAYqOjlZ2drYkKTs7W4GBgXbAkaTY2Fh5enoqJyfHrhkyZIgdcCQpLi5OBQUF+v777+2ay49TWVN5nOpcuHBBLpfLbQEAAGZq1JDjdDolSSEhIW7rQ0JC7G1Op1PBwcFu2728vBQUFORWU10blx+jpprK7dWZM2eOAgIC7CU8PLy+QwQAAG3EdXV31bRp01RaWmovJ06caOkuAQCAJtKoIcfhcEiSioqK3NYXFRXZ2xwOh4qLi922X7x4UadOnXKrqa6Ny49RU03l9ur4+PjI39/fbQEAAGZq1JATGRkph8OhrKwse53L5VJOTo5iYmIkSTExMSopKVFubq5ds3XrVlVUVCg6Otqu2bFjh8rLy+2azMxM9ezZU507d7ZrLj9OZU3lcQAAwPWt3iHnzJkzysvLU15enqQfv2ycl5en48ePy8PDQykpKXr55Ze1YcMGHTx4UGPHjlVYWJh9B1bv3r01fPhwjR8/Xnv27NHOnTuVnJysMWPGKCwsTJL0+OOPy9vbW4mJicrPz9fq1av1+uuva9KkSXY/nn/+eWVkZGj+/Pn661//qpkzZ2rfvn1KTk6+9ncFAAC0efV+QOe+fft0//33268rg8e4ceOUnp6uKVOm6OzZs5owYYJKSkp0zz33KCMjQ76+vvY+K1euVHJysoYOHSpPT0+NHj1aixYtsrcHBARoy5YtSkpK0qBBg9S1a1elpqa6/ZbOXXfdpVWrVmn69On693//d91yyy1av369+vbt26A3AgAAmOWafienreN3cszQVn8n58rj8Zs8AHB1WuR3cgAAAFoLQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI3m1dAdgph5TN7m9/mZufAv1BABwveJKDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIzk1dIdAHBtekzdVGXdN3PjW6AnANC6cCUHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIzV6yLl06ZJefPFFRUZGys/PTzfddJNmz54ty7LsGsuylJqaqtDQUPn5+Sk2NlZHjhxxa+fUqVNKSEiQv7+/AgMDlZiYqDNnzrjVfPnll7r33nvl6+ur8PBwpaWlNfZwAABAG9XoIeeVV17R0qVL9cYbb+jw4cN65ZVXlJaWpsWLF9s1aWlpWrRokZYtW6acnBx17NhRcXFxOn/+vF2TkJCg/Px8ZWZmauPGjdqxY4cmTJhgb3e5XBo2bJgiIiKUm5urefPmaebMmXrrrbcae0gAAKAN8mrsBnft2qWRI0cqPj5ektSjRw+9//772rNnj6Qfr+IsXLhQ06dP18iRIyVJ7777rkJCQrR+/XqNGTNGhw8fVkZGhvbu3avBgwdLkhYvXqwHH3xQr776qsLCwrRy5UqVlZVp+fLl8vb21q233qq8vDwtWLDALQwBAIDrU6NfybnrrruUlZWlv/3tb5Kk//mf/9EXX3yhESNGSJIKCwvldDoVGxtr7xMQEKDo6GhlZ2dLkrKzsxUYGGgHHEmKjY2Vp6encnJy7JohQ4bI29vbromLi1NBQYG+//77avt24cIFuVwutwUAAJip0a/kTJ06VS6XS7169VK7du106dIl/fGPf1RCQoIkyel0SpJCQkLc9gsJCbG3OZ1OBQcHu3fUy0tBQUFuNZGRkVXaqNzWuXPnKn2bM2eOXnrppUYYJQAAaO0aPeSsWbNGK1eu1KpVq+w/IaWkpCgsLEzjxo1r7MPVy7Rp0zRp0iT7tcvlUnh4eAv2qPXpMXWT2+tv5sa3UE8AALg2jR5yJk+erKlTp2rMmDGSpKioKB07dkxz5szRuHHj5HA4JElFRUUKDQ219ysqKtKAAQMkSQ6HQ8XFxW7tXrx4UadOnbL3dzgcKioqcqupfF1ZcyUfHx/5+Phc+yABAECr1+jfyTl37pw8Pd2bbdeunSoqKiRJkZGRcjgcysrKsre7XC7l5OQoJiZGkhQTE6OSkhLl5ubaNVu3blVFRYWio6Ptmh07dqi8vNyuyczMVM+ePav9UxUAALi+NHrIefjhh/XHP/5RmzZt0jfffKN169ZpwYIF+vnPfy5J8vDwUEpKil5++WVt2LBBBw8e1NixYxUWFqZRo0ZJknr37q3hw4dr/Pjx2rNnj3bu3Knk5GSNGTNGYWFhkqTHH39c3t7eSkxMVH5+vlavXq3XX3/d7c9RAADg+tXof65avHixXnzxRf32t79VcXGxwsLC9Jvf/Eapqal2zZQpU3T27FlNmDBBJSUluueee5SRkSFfX1+7ZuXKlUpOTtbQoUPl6emp0aNHa9GiRfb2gIAAbdmyRUlJSRo0aJC6du2q1NRUbh8HAACSmiDkdOrUSQsXLtTChQtrrPHw8NCsWbM0a9asGmuCgoK0atWqWo/Vr18/ff755w3tKgAAMBjPrgIAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMFKj30IOwFxXPttM4vlmAFovruQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEbyaukOALg+9Zi6ye31N3PjW6gnAEzFlRwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkZok5Pz973/XE088oS5dusjPz09RUVHat2+fvd2yLKWmpio0NFR+fn6KjY3VkSNH3No4deqUEhIS5O/vr8DAQCUmJurMmTNuNV9++aXuvfde+fr6Kjw8XGlpaU0xHAAA0AY1esj5/vvvdffdd6t9+/b67//+b3311VeaP3++OnfubNekpaVp0aJFWrZsmXJyctSxY0fFxcXp/Pnzdk1CQoLy8/OVmZmpjRs3aseOHZowYYK93eVyadiwYYqIiFBubq7mzZunmTNn6q233mrsIQEAgDbIq7EbfOWVVxQeHq4VK1bY6yIjI+1/W5alhQsXavr06Ro5cqQk6d1331VISIjWr1+vMWPG6PDhw8rIyNDevXs1ePBgSdLixYv14IMP6tVXX1VYWJhWrlypsrIyLV++XN7e3rr11luVl5enBQsWuIUhAABwfWr0KzkbNmzQ4MGD9Ytf/ELBwcG67bbb9Oc//9neXlhYKKfTqdjYWHtdQECAoqOjlZ2dLUnKzs5WYGCgHXAkKTY2Vp6ensrJybFrhgwZIm9vb7smLi5OBQUF+v7776vt24ULF+RyudwWAABgpkYPOf/7v/+rpUuX6pZbbtHmzZv17LPP6ne/+53eeecdSZLT6ZQkhYSEuO0XEhJib3M6nQoODnbb7uXlpaCgILea6tq4/BhXmjNnjgICAuwlPDz8GkcLAABaq0YPORUVFRo4cKD+9Kc/6bbbbtOECRM0fvx4LVu2rLEPVW/Tpk1TaWmpvZw4caKluwQAAJpIo4ec0NBQ9enTx21d7969dfz4cUmSw+GQJBUVFbnVFBUV2dscDoeKi4vdtl+8eFGnTp1yq6mujcuPcSUfHx/5+/u7LQAAwEyNHnLuvvtuFRQUuK3729/+poiICEk/fgnZ4XAoKyvL3u5yuZSTk6OYmBhJUkxMjEpKSpSbm2vXbN26VRUVFYqOjrZrduzYofLycrsmMzNTPXv2dLuTCwAAXJ8aPeRMnDhRu3fv1p/+9CcdPXpUq1at0ltvvaWkpCRJkoeHh1JSUvTyyy9rw4YNOnjwoMaOHauwsDCNGjVK0o9XfoYPH67x48drz5492rlzp5KTkzVmzBiFhYVJkh5//HF5e3srMTFR+fn5Wr16tV5//XVNmjSpsYcEAADaoEa/hfz222/XunXrNG3aNM2aNUuRkZFauHChEhIS7JopU6bo7NmzmjBhgkpKSnTPPfcoIyNDvr6+ds3KlSuVnJysoUOHytPTU6NHj9aiRYvs7QEBAdqyZYuSkpI0aNAgde3aVampqdw+DgAAJDVByJGkhx56SA899FCN2z08PDRr1izNmjWrxpqgoCCtWrWq1uP069dPn3/+eYP72Rb0mLqpyrpv5sa3QE8AAGhbeHYVAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEZqkgd0AkBrcuWDbnnILXB94EoOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEbyaukOAEBb0WPqpirrvpkb3wI9AXA1uJIDAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBITR5y5s6dKw8PD6WkpNjrzp8/r6SkJHXp0kU33HCDRo8eraKiIrf9jh8/rvj4eHXo0EHBwcGaPHmyLl686Fazbds2DRw4UD4+Prr55puVnp7e1MMBAABtRJOGnL179+rNN99Uv3793NZPnDhRH3/8sdauXavt27fr5MmTeuSRR+ztly5dUnx8vMrKyrRr1y698847Sk9PV2pqql1TWFio+Ph43X///crLy1NKSoqefvppbd68uSmHBAAA2ogmCzlnzpxRQkKC/vznP6tz5872+tLSUr399ttasGCBHnjgAQ0aNEgrVqzQrl27tHv3bknSli1b9NVXX+m9997TgAEDNGLECM2ePVtLlixRWVmZJGnZsmWKjIzU/Pnz1bt3byUnJ+vf/u3f9NprrzXVkAAAQBvSZCEnKSlJ8fHxio2NdVufm5ur8vJyt/W9evVS9+7dlZ2dLUnKzs5WVFSUQkJC7Jq4uDi5XC7l5+fbNVe2HRcXZ7dRnQsXLsjlcrktAADATE3yi8cffPCB9u/fr71791bZ5nQ65e3trcDAQLf1ISEhcjqdds3lAadye+W22mpcLpd++OEH+fn5VTn2nDlz9NJLLzV4XAAAoO1o9Cs5J06c0PPPP6+VK1fK19e3sZu/JtOmTVNpaam9nDhxoqW7BAAAmkijh5zc3FwVFxdr4MCB8vLykpeXl7Zv365FixbJy8tLISEhKisrU0lJidt+RUVFcjgckiSHw1HlbqvK13XV+Pv7V3sVR5J8fHzk7+/vtgAAADM1esgZOnSoDh48qLy8PHsZPHiwEhIS7H+3b99eWVlZ9j4FBQU6fvy4YmJiJEkxMTE6ePCgiouL7ZrMzEz5+/urT58+ds3lbVTWVLYBAACub43+nZxOnTqpb9++bus6duyoLl262OsTExM1adIkBQUFyd/fX88995xiYmJ05513SpKGDRumPn366Mknn1RaWpqcTqemT5+upKQk+fj4SJKeeeYZvfHGG5oyZYqeeuopbd26VWvWrNGmTVWfEgwAAK4/TfLF47q89tpr8vT01OjRo3XhwgXFxcXpP//zP+3t7dq108aNG/Xss88qJiZGHTt21Lhx4zRr1iy7JjIyUps2bdLEiRP1+uuvq1u3bvrLX/6iuLi4lhgSAABoZZol5Gzbts3tta+vr5YsWaIlS5bUuE9ERIQ++eSTWtu97777dODAgcboIgAAMAzPrgIAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGKlFfgwQAFC7HlOr/nr7N3PjW6AnQNvFlRwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABG8mrpDgAAmlaPqZvcXn8zN76FegI0L67kAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiVvIm8iVt2xK3LYJAEBz4koOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEbiAZ0AgEbDw4nRmnAlBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkRo95MyZM0e33367OnXqpODgYI0aNUoFBQVuNefPn1dSUpK6dOmiG264QaNHj1ZRUZFbzfHjxxUfH68OHTooODhYkydP1sWLF91qtm3bpoEDB8rHx0c333yz0tPTG3s4AACgjWr0kLN9+3YlJSVp9+7dyszMVHl5uYYNG6azZ8/aNRMnTtTHH3+stWvXavv27Tp58qQeeeQRe/ulS5cUHx+vsrIy7dq1S++8847S09OVmppq1xQWFio+Pl7333+/8vLylJKSoqefflqbN29u7CEBAIA2qNF/JycjI8PtdXp6uoKDg5Wbm6shQ4aotLRUb7/9tlatWqUHHnhAkrRixQr17t1bu3fv1p133qktW7boq6++0qeffqqQkBANGDBAs2fP1gsvvKCZM2fK29tby5YtU2RkpObPny9J6t27t7744gu99tpriouLa+xhAQCANqbJv5NTWloqSQoKCpIk5ebmqry8XLGxsXZNr1691L17d2VnZ0uSsrOzFRUVpZCQELsmLi5OLpdL+fn5ds3lbVTWVLZRnQsXLsjlcrktAADATE0acioqKpSSkqK7775bffv2lSQ5nU55e3srMDDQrTYkJEROp9OuuTzgVG6v3FZbjcvl0g8//FBtf+bMmaOAgAB7CQ8Pv+YxAgCA1qlJQ05SUpIOHTqkDz74oCkPc9WmTZum0tJSezlx4kRLdwkAADSRJnt2VXJysjZu3KgdO3aoW7du9nqHw6GysjKVlJS4Xc0pKiqSw+Gwa/bs2ePWXuXdV5fXXHlHVlFRkfz9/eXn51dtn3x8fOTj43PNYwMAAK1fo1/JsSxLycnJWrdunbZu3arIyEi37YMGDVL79u2VlZVlrysoKNDx48cVExMjSYqJidHBgwdVXFxs12RmZsrf3199+vSxay5vo7Kmsg0AAHB9a/QrOUlJSVq1apU++ugjderUyf4OTUBAgPz8/BQQEKDExERNmjRJQUFB8vf313PPPaeYmBjdeeedkqRhw4apT58+evLJJ5WWlian06np06crKSnJvhLzzDPP6I033tCUKVP01FNPaevWrVqzZo02bar6BFwAAHD9afQrOUuXLlVpaanuu+8+hYaG2svq1avtmtdee00PPfSQRo8erSFDhsjhcOjDDz+0t7dr104bN25Uu3btFBMToyeeeEJjx47VrFmz7JrIyEht2rRJmZmZ6t+/v+bPn6+//OUv3D4OAAAkNcGVHMuy6qzx9fXVkiVLtGTJkhprIiIi9Mknn9Tazn333acDBw7Uu48AAMB8PLsKAAAYqcnurgIAoKF6TK36/cpv5sa3QE/QlnElBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASNxCDgC47l15yzq3q5uBKzkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGIkHdAIA0Eh40GfrwpUcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICR+J0cAADaoCt/k0fid3muxJUcAABgJEIOAAAwEiEHAAAYiZADAACMxBePAQC4jpn8BWau5AAAACMRcgAAgJEIOQAAwEh8JwcAAFyz1vjdHq7kAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEZq8yFnyZIl6tGjh3x9fRUdHa09e/a0dJcAAEAr0KZDzurVqzVp0iTNmDFD+/fvV//+/RUXF6fi4uKW7hoAAGhhbTrkLFiwQOPHj9evf/1r9enTR8uWLVOHDh20fPnylu4aAABoYV4t3YGGKisrU25urqZNm2av8/T0VGxsrLKzs6vd58KFC7pw4YL9urS0VJLkcrkavX8VF85VWdeQ4zRWOw09XkOP1VjtNORYDT2eKe/1tbTVnMdqyXOE95rPY13H471u3XNW2a5lWbUXWm3U3//+d0uStWvXLrf1kydPtu64445q95kxY4YliYWFhYWFhcWA5cSJE7VmhTZ7Jachpk2bpkmTJtmvKyoqdOrUKXXp0kUeHh7X3L7L5VJ4eLhOnDghf3//a26vNWKMZmCMZjB9jKaPT2KMDWVZlk6fPq2wsLBa69psyOnatavatWunoqIit/VFRUVyOBzV7uPj4yMfHx+3dYGBgY3eN39/f2NP1kqM0QyM0Qymj9H08UmMsSECAgLqrGmzXzz29vbWoEGDlJWVZa+rqKhQVlaWYmJiWrBnAACgNWizV3IkadKkSRo3bpwGDx6sO+64QwsXLtTZs2f161//uqW7BgAAWlibDjmPPvqo/u///k+pqalyOp0aMGCAMjIyFBIS0iL98fHx0YwZM6r8ScwkjNEMjNEMpo/R9PFJjLGpeVhWXfdfAQAAtD1t9js5AAAAtSHkAAAAIxFyAACAkQg5AADASIScqzRz5kx5eHi4Lb169ap1n7Vr16pXr17y9fVVVFSUPvnkk2bqbcP06NGjyhg9PDyUlJRUbX16enqVWl9f32bude127Nihhx9+WGFhYfLw8ND69evdtluWpdTUVIWGhsrPz0+xsbE6cuRIne0uWbJEPXr0kK+vr6Kjo7Vnz54mGkHdahtjeXm5XnjhBUVFRaljx44KCwvT2LFjdfLkyVrbbMj53pTqmsdf/epXVfo7fPjwOtttK/MoqdrPpoeHh+bNm1djm61pHufMmaPbb79dnTp1UnBwsEaNGqWCggK3mvPnzyspKUldunTRDTfcoNGjR1f5wdcrNfQz3BTqGuOpU6f03HPPqWfPnvLz81P37t31u9/9zn6OYk0aen43hauZx/vuu69Kf5955pla222qeSTk1MOtt96q7777zl6++OKLGmt37dqlxx57TImJiTpw4IBGjRqlUaNG6dChQ83Y4/rZu3ev2/gyMzMlSb/4xS9q3Mff399tn2PHjjVXd6/K2bNn1b9/fy1ZsqTa7WlpaVq0aJGWLVumnJwcdezYUXFxcTp//nyNba5evVqTJk3SjBkztH//fvXv319xcXEqLi5uqmHUqrYxnjt3Tvv379eLL76o/fv368MPP1RBQYF+9rOf1dlufc73plbXPErS8OHD3fr7/vvv19pmW5pHSW5j++6777R8+XJ5eHho9OjRtbbbWuZx+/btSkpK0u7du5WZmany8nINGzZMZ8+etWsmTpyojz/+WGvXrtX27dt18uRJPfLII7W225DPcFOpa4wnT57UyZMn9eqrr+rQoUNKT09XRkaGEhMT62y7vud3U7maeZSk8ePHu/U3LS2t1nabbB4b4VmZ14UZM2ZY/fv3v+r6X/7yl1Z8fLzbuujoaOs3v/lNI/es6Tz//PPWTTfdZFVUVFS7fcWKFVZAQEDzduoaSLLWrVtnv66oqLAcDoc1b948e11JSYnl4+Njvf/++zW2c8cdd1hJSUn260uXLllhYWHWnDlzmqTf9XHlGKuzZ88eS5J17NixGmvqe743p+rGOG7cOGvkyJH1aqetz+PIkSOtBx54oNaa1jyPxcXFliRr+/btlmX9+Nlr3769tXbtWrvm8OHDliQrOzu72jYa+hluLleOsTpr1qyxvL29rfLy8hprGnJ+N5fqxvjTn/7Uev7556+6jaacR67k1MORI0cUFhamG2+8UQkJCTp+/HiNtdnZ2YqNjXVbFxcXp+zs7KbuZqMoKyvTe++9p6eeeqrWh5eeOXNGERERCg8P18iRI5Wfn9+Mvbw2hYWFcjqdbvMUEBCg6OjoGueprKxMubm5bvt4enoqNja2zcxtaWmpPDw86nxuW33O99Zg27ZtCg4OVs+ePfXss8/qn//8Z421bX0ei4qKtGnTpqu6AtBa57HyTzRBQUGSpNzcXJWXl7vNSa9evdS9e/ca56Qhn+HmdOUYa6rx9/eXl1ftv81bn/O7OdU0xpUrV6pr167q27evpk2bpnPnztXYRlPOIyHnKkVHR9uXFpcuXarCwkLde++9On36dLX1Tqezyi8vh4SEyOl0Nkd3r9n69etVUlKiX/3qVzXW9OzZU8uXL9dHH32k9957TxUVFbrrrrv07bffNl9Hr0HlXNRnnv7xj3/o0qVLbXZuz58/rxdeeEGPPfZYrQ/Kq+/53tKGDx+ud999V1lZWXrllVe0fft2jRgxQpcuXaq2vq3P4zvvvKNOnTrV+aec1jqPFRUVSklJ0d13362+fftK+vHz6O3tXSV81zYnDfkMN5fqxnilf/zjH5o9e7YmTJhQa1v1Pb+bS01jfPzxx/Xee+/ps88+07Rp0/Rf//VfeuKJJ2pspynnsU0/1qE5jRgxwv53v379FB0drYiICK1Zs+aq/m+qrXn77bc1YsSIWh9jHxMT4/Yw1Lvuuku9e/fWm2++qdmzZzdHN1EP5eXl+uUvfynLsrR06dJaa9va+T5mzBj731FRUerXr59uuukmbdu2TUOHDm3BnjWN5cuXKyEhoc4v+rfWeUxKStKhQ4da9HteTa2uMbpcLsXHx6tPnz6aOXNmrW211vO7pjFeHtqioqIUGhqqoUOH6uuvv9ZNN93UrH3kSk4DBQYG6ic/+YmOHj1a7XaHw1HlroCioiI5HI7m6N41OXbsmD799FM9/fTT9dqvffv2uu2222p8T1qbyrmozzx17dpV7dq1a3NzWxlwjh07pszMzFqv4lSnrvO9tbnxxhvVtWvXGvvbVudRkj7//HMVFBTU+/MptY55TE5O1saNG/XZZ5+pW7du9nqHw6GysjKVlJS41dc2Jw35DDeHmsZY6fTp0xo+fLg6deqkdevWqX379vVqv67zuznUNcbLRUdHS1Kt/72UmmYeCTkNdObMGX399dcKDQ2tdntMTIyysrLc1mVmZrpd+WitVqxYoeDgYMXHx9drv0uXLungwYM1vietTWRkpBwOh9s8uVwu5eTk1DhP3t7eGjRokNs+FRUVysrKarVzWxlwjhw5ok8//VRdunSpdxt1ne+tzbfffqt//vOfNfa3Lc5jpbfffluDBg1S//79671vS86jZVlKTk7WunXrtHXrVkVGRrptHzRokNq3b+82JwUFBTp+/HiNc9KQz3BTqmuMlf0bNmyYvL29tWHDhgb97EZd53dTupoxXikvL0+Sauxvk87jNX1t+Try+9//3tq2bZtVWFho7dy504qNjbW6du1qFRcXW5ZlWU8++aQ1depUu37nzp2Wl5eX9eqrr1qHDx+2ZsyYYbVv3946ePBgSw3hqly6dMnq3r279cILL1TZduUYX3rpJWvz5s3W119/beXm5lpjxoyxfH19rfz8/Obscq1Onz5tHThwwDpw4IAlyVqwYIF14MAB+86iuXPnWoGBgdZHH31kffnll9bIkSOtyMhI64cffrDbeOCBB6zFixfbrz/44APLx8fHSk9Pt7766itrwoQJVmBgoOV0Opt9fJZV+xjLysqsn/3sZ1a3bt2svLw867vvvrOXCxcu2G1cOca6zvfmVtsYT58+bf3hD3+wsrOzrcLCQuvTTz+1Bg4caN1yyy3W+fPn7Tba8jxWKi0ttTp06GAtXbq02jZa8zw+++yzVkBAgLVt2za38/DcuXN2zTPPPGN1797d2rp1q7Vv3z4rJibGiomJcWunZ8+e1ocffmi/vprPcHOpa4ylpaVWdHS0FRUVZR09etSt5uLFi9WO8WrP79YyxqNHj1qzZs2y9u3bZxUWFlofffSRdeONN1pDhgxxa6e55pGQc5UeffRRKzQ01PL29rb+9V//1Xr00Ueto0eP2tt/+tOfWuPGjXPbZ82aNdZPfvITy9vb27r11lutTZs2NXOv62/z5s2WJKugoKDKtivHmJKSYnXv3t3y9va2QkJCrAcffNDav39/M/a2bp999pklqcpSOY6KigrrxRdftEJCQiwfHx9r6NChVcYeERFhzZgxw23d4sWL7bHfcccd1u7du5tpRFXVNsbCwsJqt0myPvvsM7uNK8dY1/ne3Gob47lz56xhw4ZZ//Iv/2K1b9/eioiIsMaPH18lrLTleaz05ptvWn5+flZJSUm1bbTmeazpPFyxYoVd88MPP1i//e1vrc6dO1sdOnSwfv7zn1vfffddlXYu3+dqPsPNpa4x1jTHkqzCwkK3dir3udrzu7nUNcbjx49bQ4YMsYKCgiwfHx/r5ptvtiZPnmyVlpZWaac55tHj/x8MAADAKHwnBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAj/T8EToZtruIPJQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cap_lens = []\n",
        "for my_caption in reduced_train_captions:\n",
        "    for x in my_caption[1]:\n",
        "        cap_lens.append(len(x.split())-1) \n",
        "\n",
        "plt.hist(cap_lens,bins=50, rwidth=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG19"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "size_score = {}\n",
        "tokenizer_dict = {}\n",
        "model_dict = {}\n",
        "for voc_size in [5000, 6000, 6500,7500, 10000]:\n",
        "    # Use the top 10000 words for a vocabulary.\n",
        "    new_vocabulary_size = voc_size\n",
        "    tokenizer_dict[str(voc_size)] = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=new_vocabulary_size,\n",
        "        standardize=standardize,\n",
        "        ragged=True)\n",
        "    # Learn the vocabulary from the caption data.\n",
        "\n",
        "    tokenizer_dict[str(voc_size)].adapt(reduced_train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\n",
        "\n",
        "    # Create mappings for words to indices and indices to words.\n",
        "    new_word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer_dict[str(voc_size)].get_vocabulary())\n",
        "    new_index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer_dict[str(voc_size)].get_vocabulary(),\n",
        "        invert=True)\n",
        "\n",
        "    # ENABLE IF NEEDED!!!!\n",
        "    # save_dataset(reduced_train_raw, f'vgg19_voc_train_{voc_size}', vgg19, new_tokenizer)\n",
        "    # save_dataset(reduced_test_raw, f'vgg19_voc_test_{voc_size}', vgg19, new_tokenizer)\n",
        "\n",
        "    vgg19_train_ds = load_dataset(f'vgg19_voc_train_{voc_size}')\n",
        "    vgg19_test_ds = load_dataset(f'vgg19_voc_test_{voc_size}')\n",
        "\n",
        "    vgg19_output_layer = TokenOutput(tokenizer_dict[str(voc_size)], banned_tokens=('', '[UNK]', '[START]'))\n",
        "    # This might run a little faster if the dataset didn't also have to load the image data.\n",
        "    vgg19_output_layer.adapt(vgg19_train_ds.map(lambda inputs, labels: labels))\n",
        "\n",
        "    model_dict[str(voc_size)] = Captioner(tokenizer_dict[str(voc_size)], feature_extractor=vgg19, output_layer=vgg19_output_layer,\n",
        "                    units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "\n",
        "    model_dict[str(voc_size)].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "            loss=masked_loss,\n",
        "            metrics=[masked_acc])\n",
        "\n",
        "    history_model_vgg19 = model_dict[str(voc_size)].fit(\n",
        "        vgg19_train_ds.repeat(),\n",
        "        steps_per_epoch=100,\n",
        "        validation_data=vgg19_test_ds.repeat(),\n",
        "        validation_steps=20,\n",
        "        epochs=100,\n",
        "        callbacks=callbacks)\n",
        "\n",
        "    size_score[str(voc_size)] = my_corpus_blue(100,model_dict[str(voc_size)])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gensim.downloader\n",
        "# import joblib\n",
        "\n",
        "# # Show all available models in gensim-data\n",
        "# # print(list(gensim.downloader.info()['models'].keys()))\n",
        "# for i in [50, 100, 200, 300]:\n",
        "#     glove_vectors = gensim.downloader.load(f'glove-wiki-gigaword-{i}')\n",
        "#     joblib.dump(glove_vectors, f'glove_vectors{i}.plk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 9661 words (339 misses)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3282/3282 [02:03<00:00, 26.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uniform entropy: 9.21\n",
            "Marginal entropy: 5.46\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.2372 - masked_acc: 0.1810\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a man in the a a woman in a is on a wearing a man and young the in the and and of a man in a man in a man is in the a man\n",
            "a air\n",
            "\n",
            "100/100 [==============================] - 24s 197ms/step - loss: 5.2372 - masked_acc: 0.1810 - val_loss: 4.9776 - val_masked_acc: 0.2170\n",
            "Epoch 2/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.9203 - masked_acc: 0.2231\n",
            "\n",
            "a man in a man in a man in a man in a red\n",
            "a man is playing in a black\n",
            "for a in snow caucasian the push on the is man in the car beer with chair his\n",
            "\n",
            "100/100 [==============================] - 10s 105ms/step - loss: 4.9165 - masked_acc: 0.2237 - val_loss: 4.7861 - val_masked_acc: 0.2303\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7305 - masked_acc: 0.2372\n",
            "\n",
            "a man in a black dog is a black a black dog is a red\n",
            "a black shirt is is two water\n",
            "a woman playing with the sidewalk off\n",
            "\n",
            "100/100 [==============================] - 10s 98ms/step - loss: 4.7305 - masked_acc: 0.2372 - val_loss: 4.6626 - val_masked_acc: 0.2481\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6474 - masked_acc: 0.2499\n",
            "\n",
            "a man in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a blue shirt is in a\n",
            "a man in a white shirt is in children and a black shirt is with a person is in a red\n",
            "a man in a red bicycle on a green boots on his environment\n",
            "\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 4.6474 - masked_acc: 0.2499 - val_loss: 4.4758 - val_masked_acc: 0.2684\n",
            "Epoch 5/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.5132 - masked_acc: 0.2587\n",
            "\n",
            "a man in a black shirt is in a blue shirt is playing on the water\n",
            "a man in a red shirt is is water\n",
            "a group of girls is playing\n",
            "\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 4.5104 - masked_acc: 0.2590 - val_loss: 4.4500 - val_masked_acc: 0.2620\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4854 - masked_acc: 0.2649\n",
            "\n",
            "a man in a white shirt is playing a red\n",
            "a man in a black shirt is in a white shirt and white shirt in a black dog\n",
            "a man in brown her of stunts\n",
            "\n",
            "100/100 [==============================] - 10s 104ms/step - loss: 4.4854 - masked_acc: 0.2649 - val_loss: 4.2930 - val_masked_acc: 0.2782\n",
            "Epoch 7/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.3738 - masked_acc: 0.2725\n",
            "\n",
            "a man is playing on the water\n",
            "a man in a white shirt is a blue toy\n",
            "garden circle crouches of a backpack walks in the background\n",
            "\n",
            "100/100 [==============================] - 9s 94ms/step - loss: 4.3729 - masked_acc: 0.2727 - val_loss: 4.2671 - val_masked_acc: 0.2785\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2969 - masked_acc: 0.2807\n",
            "\n",
            "a man is a a black dog is standing on a water\n",
            "a man is playing a man in a brown dog is is on the water\n",
            "a children in the jacket in a standing and on a grass with the beach\n",
            "\n",
            "100/100 [==============================] - 11s 115ms/step - loss: 4.2969 - masked_acc: 0.2807 - val_loss: 4.2742 - val_masked_acc: 0.2811\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2336 - masked_acc: 0.2852\n",
            "\n",
            "a man is riding a blue shirt is riding a blue shirt is running in the water\n",
            "a man is a red shirt is sitting on a beach\n",
            "a girl in the snow\n",
            "\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 4.2336 - masked_acc: 0.2852 - val_loss: 4.1399 - val_masked_acc: 0.2890\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2000 - masked_acc: 0.2875\n",
            "\n",
            "a man is playing a water\n",
            "a man is a dirt in the water\n",
            "navy is standing in the the snow\n",
            "\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 4.2000 - masked_acc: 0.2875 - val_loss: 4.0726 - val_masked_acc: 0.2977\n",
            "Epoch 11/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.1717 - masked_acc: 0.2908\n",
            "\n",
            "a man is jumping on a water\n",
            "the boy is jumping a river\n",
            "a picnic is motorbike the ball up a group of a running brown snow citizens\n",
            "\n",
            "100/100 [==============================] - 9s 94ms/step - loss: 4.1708 - masked_acc: 0.2908 - val_loss: 4.1412 - val_masked_acc: 0.2905\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1351 - masked_acc: 0.2894\n",
            "\n",
            "a man is jumping on a beach\n",
            "a man wearing black shorts and pink along the snow\n",
            "two children walk sign under the water\n",
            "\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 4.1351 - masked_acc: 0.2894 - val_loss: 4.0474 - val_masked_acc: 0.3027\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1300 - masked_acc: 0.2910\n",
            "\n",
            "a man is jumping on a beach\n",
            "a person in a blue shirt is jumping to the water\n",
            "a man in a red shorts top of water in the water\n",
            "\n",
            "100/100 [==============================] - 10s 105ms/step - loss: 4.1300 - masked_acc: 0.2910 - val_loss: 4.0236 - val_masked_acc: 0.3001\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0743 - masked_acc: 0.2929\n",
            "\n",
            "a man is jumping on a beach\n",
            "a boy is in a park\n",
            "a child is parts the rodeo\n",
            "\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 4.0743 - masked_acc: 0.2929 - val_loss: 3.9296 - val_masked_acc: 0.3016\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0285 - masked_acc: 0.2977\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a person is jumping in the water\n",
            "a person jumps in the lake\n",
            "\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 4.0285 - masked_acc: 0.2977 - val_loss: 3.9839 - val_masked_acc: 0.3039\n",
            "Epoch 16/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.9985 - masked_acc: 0.3024\n",
            "\n",
            "a man in a blue shirt is running through a beach\n",
            "a man with a blue jacket is jumping on the water\n",
            "a little boy play\n",
            "\n",
            "100/100 [==============================] - 9s 93ms/step - loss: 3.9986 - masked_acc: 0.3021 - val_loss: 3.9378 - val_masked_acc: 0.3064\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9909 - masked_acc: 0.3023\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a boy is jumping through a black in the ocean\n",
            "a lawn in a shorts running through a blue water\n",
            "\n",
            "100/100 [==============================] - 11s 111ms/step - loss: 3.9909 - masked_acc: 0.3023 - val_loss: 3.8608 - val_masked_acc: 0.3125\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9750 - masked_acc: 0.3005\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a boy is jumping over the water\n",
            "a black young boy\n",
            "\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 3.9750 - masked_acc: 0.3005 - val_loss: 3.8559 - val_masked_acc: 0.3089\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9656 - masked_acc: 0.3044\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a surfer in a green shirt is jumping in the water\n",
            "2 arts in sand\n",
            "\n",
            "100/100 [==============================] - 9s 94ms/step - loss: 3.9656 - masked_acc: 0.3044 - val_loss: 3.8541 - val_masked_acc: 0.3095\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9235 - masked_acc: 0.3048\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a young boy is jumping on a beach\n",
            "2 the rocks a yard is putting tracks\n",
            "\n",
            "100/100 [==============================] - 10s 98ms/step - loss: 3.9235 - masked_acc: 0.3048 - val_loss: 3.8170 - val_masked_acc: 0.3137\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8912 - masked_acc: 0.3079\n",
            "\n",
            "a man is jumping on a beach\n",
            "a man is playing in the water\n",
            "a child above a playing walking down a sun bag\n",
            "\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 3.8912 - masked_acc: 0.3079 - val_loss: 3.8195 - val_masked_acc: 0.3170\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8653 - masked_acc: 0.3106\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a man in a black shirt is playing on a beach\n",
            "three men track up leaving to\n",
            "\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 3.8653 - masked_acc: 0.3106 - val_loss: 3.8469 - val_masked_acc: 0.3088\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8554 - masked_acc: 0.3094\n",
            "\n",
            "a man is jumping through a water\n",
            "a child in a blue shirt is jumping in the water\n",
            "a man putting jumping dog with a boat jump jumps into a wave\n",
            "\n",
            "100/100 [==============================] - 10s 98ms/step - loss: 3.8554 - masked_acc: 0.3094 - val_loss: 3.7973 - val_masked_acc: 0.3079\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8511 - masked_acc: 0.3081\n",
            "\n",
            "a man is jumping in the water\n",
            "a man in a blue blue jeans is jumping in a water\n",
            "the suite catches runs through in water\n",
            "\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 3.8511 - masked_acc: 0.3081 - val_loss: 3.7823 - val_masked_acc: 0.3129\n",
            "Epoch 25/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.8255 - masked_acc: 0.3116\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a young girl in a blue shirt is snowboarding into the water\n",
            "a man selling swinging out of a ocean calm through\n",
            "\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 3.8252 - masked_acc: 0.3118 - val_loss: 3.6922 - val_masked_acc: 0.3218\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8103 - masked_acc: 0.3113\n",
            "\n",
            "a man in a blue shirt is jumping on a beach\n",
            "a man in a yellow shirt jumping on a beach\n",
            "a man jumping on a dirt camera\n",
            "\n",
            "100/100 [==============================] - 9s 95ms/step - loss: 3.8103 - masked_acc: 0.3113 - val_loss: 3.6937 - val_masked_acc: 0.3216\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7930 - masked_acc: 0.3152\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a man in a red shirt is running on a beach\n",
            "a boy rides a woman in the surfboard next to a stick\n",
            "\n",
            "100/100 [==============================] - 11s 109ms/step - loss: 3.7930 - masked_acc: 0.3152 - val_loss: 3.7020 - val_masked_acc: 0.3251\n",
            "Epoch 28/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.7727 - masked_acc: 0.3152\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a boy in a black shirt runs through the ocean\n",
            "a boy with a red dog jumping off a beach\n",
            "\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 3.7724 - masked_acc: 0.3155 - val_loss: 3.6438 - val_masked_acc: 0.3265\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7586 - masked_acc: 0.3170\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a man rides a wave\n",
            "five people jumping down a boat course\n",
            "\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 3.7586 - masked_acc: 0.3170 - val_loss: 3.6056 - val_masked_acc: 0.3297\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7341 - masked_acc: 0.3167\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a surfer in a white shirt and a toy on a snowy hill\n",
            "a woman stuck in a colorful with a earth\n",
            "\n",
            "100/100 [==============================] - 10s 104ms/step - loss: 3.7341 - masked_acc: 0.3167 - val_loss: 3.6883 - val_masked_acc: 0.3256\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7504 - masked_acc: 0.3173\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a man in a blue shirt is jumping a swimming through the water\n",
            "a snowboarder child on a beach sand\n",
            "\n",
            "100/100 [==============================] - 10s 101ms/step - loss: 3.7504 - masked_acc: 0.3173 - val_loss: 3.6482 - val_masked_acc: 0.3246\n",
            "Epoch 32/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.7218 - masked_acc: 0.3165\n",
            "\n",
            "a man is jumping in the water\n",
            "a man is on the ocean with a boat in the water\n",
            "a man and a child on the beach in the chest\n",
            "\n",
            "100/100 [==============================] - 10s 96ms/step - loss: 3.7204 - masked_acc: 0.3167 - val_loss: 3.6799 - val_masked_acc: 0.3226\n",
            "Epoch 33/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.6820 - masked_acc: 0.3221\n",
            "\n",
            "a man in a blue shirt is jumping into the water\n",
            "a boy is jumping off of water\n",
            "a person does rider on some ocean in a wave\n",
            "\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 3.6818 - masked_acc: 0.3223 - val_loss: 3.6421 - val_masked_acc: 0.3240\n",
            "Epoch 34/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.6388 - masked_acc: 0.3238\n",
            "\n",
            "a man in a blue shirt and a black shirt is running through the water\n",
            "a man in a white shirt is swimming in the water\n",
            "car on sand surrounded by the water\n",
            "\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 3.6418 - masked_acc: 0.3238 - val_loss: 3.6069 - val_masked_acc: 0.3349\n"
          ]
        }
      ],
      "source": [
        "dict_embs = {}\n",
        "for embedding_dim in [300]:\n",
        "  # set new emb matrix\n",
        "  glove_vectors = joblib.load(f'glove_vectors{embedding_dim}.plk')\n",
        "  \n",
        "  voc_size = 10000\n",
        "  mytokenizer = tf.keras.layers.TextVectorization(\n",
        "      max_tokens=voc_size,\n",
        "      standardize=standardize,\n",
        "      ragged=True)\n",
        "  # Learn the vocabulary from the caption data.\n",
        "\n",
        "  mytokenizer.adapt(reduced_train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\n",
        "\n",
        "  myvoc = mytokenizer.get_vocabulary()\n",
        "\n",
        "  myword_index = dict(zip(myvoc, range(len(myvoc))))\n",
        "  hits = 0\n",
        "  misses = 0\n",
        "  # Prepare embedding matrix\n",
        "\n",
        "  temp_embedding_matrix = np.zeros((len(myvoc), embedding_dim))\n",
        "  for word, i in myword_index.items():\n",
        "      try:\n",
        "          embedding_vector = glove_vectors.get_vector(word)\n",
        "          temp_embedding_matrix[i] = embedding_vector\n",
        "          hits += 1\n",
        "      except:\n",
        "          misses += 1\n",
        "  print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "  # recreate the class\n",
        "  class SeqEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, max_length, depth):\n",
        "      super().__init__()\n",
        "      self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "      \n",
        "      self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        embeddings_initializer=tf.keras.initializers.Constant(temp_embedding_matrix),\n",
        "        trainable=False,\n",
        "        mask_zero=True\n",
        "      )\n",
        "\n",
        "      self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, seq):\n",
        "      seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "      x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "      x = x[tf.newaxis, :]  # (1, seq)\n",
        "      x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "      return self.add([seq,x])\n",
        "\n",
        "  vgg19_train_ds = load_dataset(f'vgg19_voc_train_{voc_size}')\n",
        "  vgg19_test_ds = load_dataset(f'vgg19_voc_test_{voc_size}')\n",
        "\n",
        "  vgg19_output_layer = TokenOutput(mytokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "  # This might run a little faster if the dataset didn't also have to load the image data.\n",
        "  vgg19_output_layer.adapt(vgg19_train_ds.map(lambda inputs, labels: labels))\n",
        "\n",
        "  my_model = Captioner(mytokenizer, feature_extractor=vgg19, output_layer=vgg19_output_layer,\n",
        "                  units=embedding_dim, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "\n",
        "  my_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "          loss=masked_loss,\n",
        "          metrics=[masked_acc])\n",
        "\n",
        "  history_model_vgg19 = my_model.fit(\n",
        "      vgg19_train_ds.repeat(),\n",
        "      steps_per_epoch=100,\n",
        "      validation_data=vgg19_test_ds.repeat(),\n",
        "      validation_steps=20,\n",
        "      epochs=100,\n",
        "      callbacks=callbacks)\n",
        "\n",
        "  dict_embs[f'{embedding_dim}'] = my_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_emb_dict = {}\n",
        "for embs, model_emb in dict_embs.items():\n",
        "    corpus_emb_dict[embs] = [my_corpus_blue(100, model_emb) for _ in range(5)] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.060443485147153586\n",
            "0.07691691386702593\n",
            "0.0863584642767384\n",
            "0.08710975050289485\n"
          ]
        }
      ],
      "source": [
        "for mine in corpus_emb_dict:\n",
        "    print(np.mean(corpus_emb_dict[mine]))\n",
        "# 300 best score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.08685727589651233"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_corpus_blue(4000, dict_embs['300'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAVE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dict_embs['300'].save_weights('./models/emb300',save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': mytokenizer.get_config(),\n",
        "#              'weights': mytokenizer.get_weights()}\n",
        "#             , open(f'./tokenizers/tokenizer10k_emb300.pkl', \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from_disk = pickle.load(open(f'./tokenizers/tokenizer10k_emb300.pkl', \"rb\"))\n",
        "customtokenizer = tf.keras.layers.TextVectorization.from_config(from_disk['config'])\n",
        "customtokenizer.set_weights(from_disk['weights'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3282/3282 [01:45<00:00, 31.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uniform entropy: 9.21\n",
            "Marginal entropy: 5.46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "vgg19_train_ds = load_dataset(f'vgg19_voc_train_10000')\n",
        "vgg19_test_ds = load_dataset(f'vgg19_voc_test_10000')\n",
        "\n",
        "vgg19_output_layer = TokenOutput(customtokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "vgg19_output_layer.adapt(vgg19_train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1dc18b3dd30>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model300 = Captioner(customtokenizer, feature_extractor=vgg19, output_layer=vgg19_output_layer,\n",
        "                  units=300, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "\n",
        "model300.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "model300.load_weights('./models/emb300').expect_partial()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------t=0.0---------------------------\n",
            "a man in a blue shirt is jumping in the water\n",
            "\n",
            "------------------------t=0.1---------------------------\n",
            "a man in a blue shirt is jumping in the water\n",
            "\n",
            "------------------------t=0.2---------------------------\n",
            "a man in a blue shirt is jumping a wave\n",
            "\n",
            "------------------------t=0.3---------------------------\n",
            "a man in a black shirt and a red shirt is jumping in the water\n",
            "\n",
            "------------------------t=0.4---------------------------\n",
            "a man in a blue shirt is jumping on the water\n",
            "\n",
            "------------------------t=0.5---------------------------\n",
            "a man jumps in a pool\n",
            "\n",
            "------------------------t=0.6---------------------------\n",
            "a man in a black shirt is surfing in a blue green shirt\n",
            "\n",
            "------------------------t=0.7---------------------------\n",
            "a man in a red shorts is surfing through the beach\n",
            "\n",
            "------------------------t=0.8---------------------------\n",
            "a child jumping from the blue water\n",
            "\n",
            "------------------------t=0.9---------------------------\n",
            "a man in an orange is is jumping in the water\n",
            "\n",
            "------------------------t=1.0---------------------------\n",
            "a black man runs through a telescopes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for t in range(11):\n",
        "    print(f'------------------------t={t/10}---------------------------')\n",
        "    print(model300.simple_gen(image, temperature=t/10))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2047289840044857\n",
            "0.21501553113191715\n",
            "0.22102465962286585\n",
            "0.22138248370279662\n",
            "0.17059181232353315\n",
            "0.18630118046954333\n",
            "0.17446750330394625\n",
            "0.15746289009185122\n",
            "0.11803161482663217\n",
            "0.11216610997005902\n",
            "0.08778161902574468\n"
          ]
        }
      ],
      "source": [
        "for t in range(11):\n",
        "    print(my_corpus_blue(100, model300, mytemperature=t/10))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5bwwk4uxRz6A"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2d38854ce3616b6787982ff660a67931f51b548d01949786d56c7578c734a1a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
